{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = (\"/Users/maukanmir/Documents/Machine-Learning/Web-Scraping-Code/Player-Salaries/dot.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "TABLE_NAME = \"team_salaries\"\n",
    "\n",
    "engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_info(html_content, year):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    player_data = []\n",
    "    players = soup.find_all('li', class_='list-group-item')\n",
    "\n",
    "    for player in players:\n",
    "        data = {}\n",
    "\n",
    "        name_div = player.find('div', class_='link')\n",
    "        salary_span = player.find('span', class_='medium')\n",
    "        team_position_small = player.find('small')\n",
    "        \n",
    "        if name_div:\n",
    "            \n",
    "            data['player'] = name_div.text.strip()\n",
    "\n",
    "            block = team_position_small.text.strip().split(\",\")\n",
    "            team = block[0]\n",
    "            pos = block[1]\n",
    "            data['team'] = team\n",
    "            data[\"pos\"] = pos\n",
    "            salary = salary_span.text.strip().replace(\"$\", \"\").replace(\",\", \"\")\n",
    "            data['salary'] = int(salary)\n",
    "            player_data.append(data)\n",
    "    \n",
    "    df = pd.DataFrame(player_data)\n",
    "    df[\"season\"] = int(year)\n",
    "    return df\n",
    "\n",
    "def extract_team_info(html_content, year):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tds = soup.findAll(\"td\")\n",
    "    stats = [td.text.strip() for td in tds]\n",
    "    sep_blocks = \" \".join(stats).split(\".\")[1:]\n",
    "    teams_salaries = [block.strip().split(\" \")[:4] for block in sep_blocks]\n",
    "    \n",
    "    df = []\n",
    "    for salary in teams_salaries:\n",
    "        teams = {}\n",
    "        for idx, part in enumerate(salary):\n",
    "            part = part.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "            if part.isalpha():\n",
    "                if salary[idx+1].isalpha():\n",
    "                    teams[\"Team\"] = part + \" \" + salary[idx+1]\n",
    "                elif not salary[idx+1].isalpha() and not salary[idx-1].isalpha():\n",
    "                    teams[\"Team\"] = part\n",
    "            elif part.isnumeric() and salary[idx-1].isalpha():\n",
    "                teams[\"salary\"] = part\n",
    "            if teams and teams not in df:\n",
    "                df.append(teams)\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"season\"] = year\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grabbing Player Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(2011, 2025)]\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "  url = f\"https://www.spotrac.com/nba/rankings/player/_/year/{year}/sort/cash_total\"\n",
    "  response = requests.get(url)\n",
    "  df = extract_player_info(response.text, year)\n",
    "  all_data = pd.concat([all_data, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['id'] = range(1, len(all_data) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save To Posgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to the database.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  with psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    ) as conn:\n",
    "      with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME};\")\n",
    "\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE {TABLE_NAME} (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                player VARCHAR(255),\n",
    "                team VARCHAR(12),\n",
    "                pos  VARCHAR(20),\n",
    "                salary INTEGER,\n",
    "                season INTEGER\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        insert_query = f'INSERT INTO {TABLE_NAME} (id, player, team, pos, salary, season) VALUES (%s, %s, %s, %s, %s, %s)'\n",
    "        rows_to_insert = [(row[\"id\"], row['player'], row['team'], row['pos'], row['salary'], row['season']) for index, row in all_data.iterrows()]\n",
    "        cur.executemany(insert_query, rows_to_insert)\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Data written to the database.\")\n",
    "except Exception as e:\n",
    "  print(f\"Database Failed to upload Data. The error is: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Team Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) + \"-\" + str(year+1) for year in range(1990, 2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame()\n",
    "for year in years:\n",
    "  base_url = f\"https://hoopshype.com/salaries/{year}/\"\n",
    "  response = requests.get(url)\n",
    "  df = extract_team_info(response.text, year)\n",
    "  all_data = pd.concat([all_data, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_data.to_sql(TABLE_NAME, engine, if_exists='replace', index=False)\n",
    "    print(\"Data successfully written to the database.\")\n",
    "except Exception as e:\n",
    "    print(f\"Database operation failed. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
